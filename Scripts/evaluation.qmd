---
title: "Evaluation and Confusion Matrix"
format: html
---

## Evaluation

```{python}
# Load libraries
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Evaluate models
for name, grid in grids.items():
    y_pred = grid.predict(X_test)
    print(f"--- {name} ---")
    print(classification_report(y_test, y_pred))
    
    cm = confusion_matrix(y_test, y_pred, labels=grid.classes_)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=grid.classes_, yticklabels=grid.classes_)
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()


# Feature importances (Random Forest)
rf = grids["Random Forest"].best_estimator_
importances = rf.named_steps["model"].feature_importances_
feature_names = rf.named_steps["preprocessor"].get_feature_names_out()
fi_df = pd.DataFrame({"Feature": feature_names, "Importance": importances}).sort_values("Importance", ascending=False)

sns.barplot(data=fi_df.head(15), x="Importance", y="Feature")
plt.title("Top Feature Importances (Random Forest)")
plt.tight_layout()
plt.show()
```